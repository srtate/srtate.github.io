<!DOCTYPE html>
<html lang="en">

    <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="Stephen R. Tate">

    <title>Readings on Randomness -- Randomness</title>

    <!-- Bootstrap CSS CDN -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/css/bootstrap.min.css" integrity="sha384-9gVQ4dYFwwWSjIDZnLEWnxCjeSWFphJiwGPXr1jddIhOegiu1FwO5qRGvFXOdJZ4" crossorigin="anonymous">

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="../css/class_srtate.css" rel="stylesheet">


    <script type="text/x-mathjax-config"> 
       MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 94, linebreaks: { automatic: true } }
		});
    </script>

  </head>


  <body>

        <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark uncg-navbar fixed-top">
      <div class="container">
        <a class="navbar-brand" href="https://www.uncg.edu/">
          <img class="collapse navbar-collapse" src="../img/uncg_class.png" alt="UNC Greensboro">
          <img class="navbar-toggler" src="../img/uncg_mobile.png" alt="UNCG">
        </a>
        <div class="navbar-brand d-flex flex-column text-left"
	     style="line-height: 1.2;">
          <span>CSC 481/681</span>
          <span class="small collapse navbar-collapse"><em>Principles of Computer Security</em></span>
        </div>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link " href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link " href="syllabus.html">Syllabus</a>
            </li>
            <li class="nav-item">
              <a class="nav-link " href="schedule.html">Schedule</a>
            </li>
            <li class="nav-item">
              <a class="nav-link " href="assignments.html">Assignments</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://uncg.instructure.com/">Canvas</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>


    <!-- Page Content -->
    <div class="container">
   
        <h3 id="randomness">2. Randomness</h3>
<p>Randomness, and generating random numbers, is one of the most important tools for building secure systems. Why should random, meaningless numbers be important? There are two important things we get from randomness:</p>
<ul>
<li><p><strong>Unpredictability:</strong> In the previous section, we considered a scenario in which an attacker performs a brute-force attack to find a <span class="math inline">\(k\)</span>-bit encryption key. The attacker doesn’t know anything about the key other than its length, and the key is equally likely to be any of the <span class="math inline">\(2^k\)</span> different possibilities. Since there is no way to predict what the actual key is, the attacker’s only way to find the key is to try all possible values (we’re ignoring the possibility of cryptanalysis, a topic that will have to wait for another time). This unpredictability comes from randomness, and while picking a 128-bit key using a good source of random bits gives a very unpredictable value, an unfortunately common cause of security weaknesses involves programmers taking shortcuts and using poor sources of randomness. For example, initial implementations of the secure sockets layer (SSL) protocol for protecting web-browsing were <a href="https://people.eecs.berkeley.edu/~daw/papers/ddj-netscape.html">very insecure because of precisely this problem</a>. A similar high-impact vulnerability in Debian’s OpenSSL package was <a href="https://www.schneier.com/blog/archives/2008/05/random_number_b.html">found in 2008</a>. There are even situations where predictable random number generators allowed people to cheat against electronic gambling machines, such as the 1995 incident in which a computer technician named Ron Harris used his knowledge of how Keno machines worked to (attempt to) <a href="https://www.casino.org/blog/keno-cheater-got-busted/">cheat a casino out of hundreds of thousands of dollars</a>. Finally, passwords should be unpredictable so that they can’t be guessed, and while passwords are generally picked by people in decidedly non-random ways, we can gain a lot of insight into security by modeling password selection as a random process so that it can be analyzed for predictability.</p></li>
<li><p><strong>High-probability uniqueness:</strong> There are many situations in security and cryptography in which we need to use some value that has not been used before by a system. For example, some authentication techniques are “challenge-response” protocols, where one side sends a challenge – sometimes called a <em>nonce</em> – to the side that needs to authenticate, and the responder must create a response that uses that nonce. If the nonce has been used before, and an attacker captures the response, they can break this protocol by simply replaying the previous response (this is called a “replay attack,” for obvious reasons). As another example, an encryption scheme called “AES in Counter Mode,” abbreviated AES-CTR, is very secure if an initial counter value has never been used before – but if we repeat an initial counter value, it becomes very, very insecure.</p>
<p>One solution would be to keep track of values that have been used before, to guarantee that values are unique. However, this requires keeping track of a potentially large amount of information, and is even more impractical if values are used across multiple systems that would need to coordinate their values. Fortunately, randomness provides a practical solution: If you pick random values from a large enough sample space, then the probability that you see the same value more than once is negligible. How large is “large enough”? The exact analysis is related to a mathematical problem known as the “birthday problem,” which we’ll look at in detail below. For now, know that you could pick a million random 128-bit values every second for 16 years, and the probability that you get the same value more than once would be minuscule (less than one in a billion).</p></li>
</ul>
<p>The primary tools we have for talking about randomness come from probability theory, so we review some of the basics of probability theory before moving on.</p>
<h4 id="a-quick-probability-theory-review">2.1 A Quick Probability Theory Review</h4>
<p>The coverage of probability theory here is just a brief review to jog your memory, since students in upper-level computer science classes like this one have seen basic probability theory in several classes before now (e.g., at UNCG you should have seen this in at least CSC 350 and STA 271/290, but you also probably did some basic probability theory in high school as well). While the concepts are well-established and standard, notation varies slightly depending on the author, and for consistency we will use the same notation that Hein used in the textbook that we use for CSC 250 and CSC 350. If you took those classes here at UNCG, then you can refer back to that book for additional information.</p>
<p>A set of possible outcomes of a random process or experiment is called a <em>sample space</em>. We focus on finite sample spaces in this class, which can be denoted as a finite set <span class="math inline">\(S=\{x_1,x_2,x_3,\cdots,x_n\}\)</span>. Each element <span class="math inline">\(x_i\)</span> of <span class="math inline">\(S\)</span> is referred to as a <em>point</em> in sample space <span class="math inline">\(S\)</span>. A <em>probability distribution</em> on <span class="math inline">\(S\)</span> is a function <span class="math inline">\(P:S\rightarrow[0,1]\)</span> that maps each point <span class="math inline">\(x_i\in S\)</span> to a real number <span class="math inline">\(P(x_i)\)</span> between 0 and 1, called the <em>probability</em> of <span class="math inline">\(x_i\)</span>, subject to the condition that the sum of all probabilities is equal to one:</p>
<p><span class="math display">\[ \sum_{i=1}^n P(x_i) = 1 . \]</span></p>
<p>One of the most common probability distributions we see is the <em>uniform distribution</em>, where every point in the sample space has probability <span class="math inline">\(1/n\)</span>. We’ll denote this probability distribution by <span class="math inline">\(U_n\)</span>, so <span class="math inline">\(U_n(x_i)=1/n\)</span> for all <span class="math inline">\(x_i\in S\)</span>. Notice that the probability doesn’t depend on the actual sample point at all – it is the same for every point. The uniform distribution comes up in many, many problems, from flipping a coin (a uniform distribution on two outcomes) to rolling a fair 6-sided die (a uniform distribution on six outcomes) to selecting a card from a fairly shuffled deck of playing cards (a uniform distribution on 52 outcomes). When we say we “pick a value at random,” we always mean that we pick using a uniform probability distribution unless we explicitly state that we’re using a different distribution. For example, if we say that we pick a 128-bit symmetric encryption key at random, that means that each of the <span class="math inline">\(2^{128}\)</span> possible keys is equally likely.</p>
<p>An <em>event</em> is some subset of points from the sample space. If <span class="math inline">\(A\subseteq S\)</span> is an event, we define the probability of the event, written <span class="math inline">\(P( A)\)</span>, as the sum of the probabilities of the points that make up the event. In other words,</p>
<p><span class="math display">\[ P(A) = \sum_{x\in A} P(x) . \]</span></p>
<p>Since events are defined as sets of sample points, we can perform the usual set operations on them. If we have two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we can combine events to represent new events — for example:</p>
<ul>
<li><p>The event <span class="math inline">\(A\cup B\)</span> represents the event that the outcome is in <em>either</em> <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> (or both);</p></li>
<li><p>The event <span class="math inline">\(A\cap B\)</span> represents the event that the outcome is in <em>both</em> <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p></li>
</ul>
<p>The intersection operator allows us to define one more fundamental property of events: <em>independence</em>. We say that events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>independent</em> if <span class="math inline">\(P(A\cap B)=P(A)P(B)\)</span>.</p>
<hr />
<p><strong>Example 2.1:</strong> Consider three tosses of a fair coin, recording heads (H) or tails (T) for each toss. The sample space is the set of eight possible outcomes: <span class="math display">\[ S=\{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT \} . \]</span></p>
<p>Since it’s a “fair coin,” the probability distribution is the uniform distribution on these eight points, so for every <span class="math inline">\(x\in S\)</span>, we have <span class="math inline">\(P(x)=U_8(x)=1/8\)</span>. There are many ways we can define events on this sample space, such as the event of “all heads” or “one head” or “more heads than tails.” Let’s define event <span class="math inline">\(A\)</span> to be “more heads than tails,” so <span class="math inline">\(A=\{HHH,HHT,HTH,THH\}\)</span>. Since there are four points in that event, and each point has probability <span class="math inline">\(1/8\)</span>, adding up over all of them we get <span class="math inline">\(P(A)=4/8=1/2\)</span>. In other words, there’s a 50-50 chance of getting more heads than tails (and a 50-50 chance of getting more tails than heads). This will be true for any odd number of coin tosses, where there can’t be a tie.</p>
<p>Now consider events <span class="math inline">\(E_1=\{HHH,HHT,HTH,HTT\}\)</span> (representing the first toss coming up heads) and <span class="math inline">\(E_2=\{HHH,HHT,THH,THT\}\)</span> (representing the second toss coming up heads), and note that <span class="math inline">\(P(E_1)=P(E_2)=1/2\)</span>. Then consider <span class="math inline">\(E_1\cap E_2=\{HHH,HHT\}\)</span>: with 2 possibilities we have <span class="math inline">\(P(E_1\cap E_2)=2/8=1/4\)</span>. Since <span class="math inline">\(P(E_1\cap E_2)=P(E_1)P(E_2)\)</span>, these events are independent</p>
<p>On the other hand, consider <span class="math inline">\(E_3=\{TTH,TTT\}\)</span> (“neither of the first two tosses is heads”) and <span class="math inline">\(E_4=\{HTT,TTT\}\)</span> (“neither of the last two tosses is heads”). Now we have <span class="math inline">\(P(E_3)=P(E_4)=1/4\)</span>, and so <span class="math inline">\(P(E_3)P(E_4)=1/16\)</span>. But <span class="math inline">\(P(E_3\cap E_4)=P(\{TTT\})=1/8\)</span>. Since <span class="math inline">\(P(E_3\cap E_4)\neq P(E_3)P(E_4)\)</span>, these events aren’t independent. Using an intuitive notion of “independence” this makes sense: If neither of the first two tosses are heads, you know that the second toss is tails, meaning that the probability of the <em>second</em> two tosses being tails being higher than if you knew nothing about the second toss (this “intuitive argument” actually argues about conditional probabilities, which gives an equivalent definition of independence that we won’t go into here).</p>
<hr />
<p>A <em>random variable</em> <span class="math inline">\(V\)</span> is a function <span class="math inline">\(V:S\rightarrow\mathbb{R}\)</span> that maps each sample space point to a real number. Typically (but not always) a random variable represents some measurement taken on a random experiment, such as the number of “heads” results in <span class="math inline">\(n\)</span> coin tosses, or the running time of an algorithm that makes random choices while it is running (in this case, the sample space is the set of all possible sequences of random choices made by the algorithm).</p>
<p>The <em>expected value</em> of a random variable <span class="math inline">\(V\)</span>, written <span class="math inline">\(E[V]\)</span>, is just the weighted sum of the random variable values, with weights given by the probabilities. More specifically,</p>
<p><span class="math display">\[ E[V]=\sum_{x\in S} P(x)\cdot V(x) . \]</span></p>
<p>While this definition of expectation can certainly be used, in practice we often view and calculate this differently. In particular, if we use <span class="math inline">\(V(S)\)</span> to represent the set of all possible random variable values, then for each value <span class="math inline">\(v\in V(S)\)</span> we can define an event for “<span class="math inline">\(V=v\)</span>” that gives all sample space points for which the value of <span class="math inline">\(V\)</span> is <span class="math inline">\(v\)</span>:</p>
<p><span class="math display">\[ (V=v) = \{ x\,|\, x\in S \mbox{ and } V(x)=v \} . \]</span></p>
<p>Now we can talk about <span class="math inline">\(P(V=v)\)</span>, the probability that <span class="math inline">\(V\)</span> has value <span class="math inline">\(v\)</span>. The following formula, the weighted sum over distinct random variable values, is how we usually compute an expected value:</p>
<p><span class="math display">\[ E[V] = \sum_{v\in V(S)} P(V=v)\cdot v . \]</span></p>
<hr />
<p><strong>Example 2.2:</strong> Continuing our example from above, we can define random variable <span class="math inline">\(X\)</span> to be the number of heads that came up in our three coin tosses, so the possible random variable values are <span class="math inline">\(X(S)=\{0,1,2,3\}\)</span>. More specifically, we have three events:</p>
<p><span class="math display">\[ \begin{array}{l}
   (X=0)=\{ TTT \}\\
   (X=1)=\{ HTT, THT, TTH \}\\
   (X=2)=\{ HHT, HTH, THH \}\\
   (X=3)=\{ HHH \} . \end{array} \]</span></p>
<p>Computing the expected value of <span class="math inline">\(X\)</span>, we get</p>
<p><span class="math display">\[ \begin{array}{rl}
E[X] &amp; = P(X=0)\cdot 0 + P(X=1)\cdot 1 + P(X=2)\cdot 2 + P(X=3)\cdot 3 \\
        &amp; = \frac{1}{8}\cdot 0 + \frac{3}{8}\cdot 1 + \frac{3}{8}\cdot 2 + \frac{1}{8}\cdot 3 \\
    &amp; = \frac{3}{8}+\frac{6}{8}+\frac{3}{8} \\
    &amp; = \frac{12}{8} \\
    &amp; = \frac{3}{2} .
\end{array} \]</span></p>
<p>This example illustrates that the every-day English language usage of “what we expect” is different from the mathematical notion of an expected value. The mathematical definition of expectation tells us that the “expected value” of the number of heads is <span class="math inline">\(3/2\)</span>, even though a fractional number of heads is not possible. So we “expect” something impossible! You get better intuition by thinking about this as an average value, and in fact the “<a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a>” says that if we perform this experiment (three coin tosses) many, many times, counting each time how many heads come up, then the average over our experiments will be very close to the expected value. So while averages and expected values are different mathematically (one is a property of experiments and the other is a property of a probability distribution), they are related in a very strong way.</p>
<hr />
<h4 id="the-birthday-problem">2.2 The Birthday Problem</h4>
<p>The “Birthday Problem,” or “Birthday Paradox,” is a common example in probability theory, and is fun to use since at first the result often runs counter to a student’s intuition. While the standard math problem is stated using small numbers and is a problem that you can visualize easily, the exact same problem — but with much larger numbers — comes up repeatedly in cryptography. This is the core problem related to the “high-probability uniqueness” use of randomness that was described at the beginning of this section, and we also see this when analyzing collisions in cryptographic hash functions (while not actually random, the same principles apply).</p>
<p>The basic “Birthday Problem” is this: Given a room with 23 people in it, what is the probability that two share the same birthday? It seems like with such a small number of people, and so many different possible birthdays, the probability should be low. However, the probability turns out to be a little more than 1/2, so it is more likely than not that two people share the same birthday!</p>
<p>To analyze this situation, we generalize the problem to <span class="math inline">\(n\)</span> people in a room, each with one of <span class="math inline">\(d\)</span> possible birthdays chosen uniformly and independently. For our specific problem we have <span class="math inline">\(n=23\)</span> and <span class="math inline">\(d=365\)</span> (ignoring leap years). Are birthdays really uniformly distributed? Not exactly, but it’s close enough!</p>
<p>Let <span class="math inline">\(D_1\)</span> represent the event that the first person’s birthday is on January 1, and <span class="math inline">\(D_2\)</span> be the event that the second person’s birthday is on January 2. Since birthdays are independent, with probability 1/365 of being on any particular day, we know that the probability of “Person 1” having a January 1 birthday <em>and</em> “Person 2” having a January 2 birthday is <span class="math inline">\(P(D_1\cap D_2)=P(D_1)P(D_2)=1/365^2\)</span>. This argument generalizes easily to any list of days and any number of people: for <span class="math inline">\(n\)</span> people and any list of <span class="math inline">\(n\)</span> days, the probability that the <span class="math inline">\(n\)</span> people have exactly those <span class="math inline">\(n\)</span> birthdays is <span class="math inline">\(1/365^n\)</span>. Since every different list of <span class="math inline">\(n\)</span> days is a distinct point in our sample space, we can pick any set <span class="math inline">\(A\)</span> of birthday assignments, and the probability that randomly assigned birthdays come from this set is exactly <span class="math inline">\(|A|/365^n\)</span>. What we’re going to do next is let <span class="math inline">\(A\)</span> be the set of all birthday assignments in which <em>no two people share a birthday</em>, and calculate the size of <span class="math inline">\(A\)</span> so that we can find the probability that no two people share a birthday.</p>
<p>Calculating <span class="math inline">\(|A|\)</span> is now just a combinatorics problem: “Person 1” has 365 different choices for their birthday. “Person 2” can pick any one of the 364 remaining days. “Person 3” has 363 options, and so on. If there were 5 people, then the number of possible birthday assignments with distinct days would be <span class="math inline">\(365\cdot 364\cdot 363\cdot 362\cdot 361\)</span>. For <span class="math inline">\(n\)</span> people, the number of birthday assignments with distinct days is <span class="math inline">\(365\cdot 364\cdots (365-n+1)\)</span>. Therefore, for <span class="math inline">\(n=23\)</span> people, the probability that all have distinct birthdays is</p>
<p><span class="math display">\[ \frac{365\cdot 364\cdot 363\cdots 344\cdot 343}{365^{23}}
   \approx 0.4927 . \]</span></p>
<p>Looked at from the opposite side, the probability that two people <em>do</em> share a birthday is approximately <span class="math inline">\(1-0.4927=0.5073\)</span>, or slightly better than <span class="math inline">\(1/2\)</span>.</p>
<p>Of course, we’re not really interested in birthdays, as unexpected and entertaining as that result is. For arbitrary <span class="math inline">\(n\)</span> and <span class="math inline">\(d\)</span>, given a uniformly-distributed sample space of size <span class="math inline">\(d\)</span>, the probability that <span class="math inline">\(n\)</span> independently chosen points contains a duplicated value is</p>
<p><span class="math display">\[ 1 - \frac{d!}{(d-n)!}\cdot\frac{1}{d^n} . \]</span></p>
<p>This formula is easy enough to calculate with a program when <span class="math inline">\(d\)</span> and <span class="math inline">\(n\)</span> are small, like the traditional birthday problem, but an example application in cryptography (which we’ll see below) can use very large values such as <span class="math inline">\(d=2^{128}\)</span> and <span class="math inline">\(n=2^{64}\)</span>. Computing those factorials exactly is simply impossible, so we need an easier-to-use formula.</p>
<p>For the values of <span class="math inline">\(d\)</span> and <span class="math inline">\(n\)</span> that we’re interested in, the probability of a duplicate can be closely approximated by <span class="math inline">\(1-e^{-n^2/(2d)}\)</span>. If we evaluate this formula with <span class="math inline">\(d=23\)</span> and <span class="math inline">\(n=365\)</span>, we get approximately 0.5155, which is accurate to within 0.01. When <span class="math inline">\(n^2/(2d)\)</span> is very small (unlike in the actual birthday problem), we can further approximate the probability of a duplicate simply as <span class="math inline">\(n^2/(2d)\)</span> – see the example below for a brief explanation of why this is so, as well as an application.</p>
<hr />
<p><strong>Example 2.3:</strong> Returning to a problem from the beginning of this section: There are systems that require unique values, and if values are ever repeated then security fails. If the values are from a large enough space, say 128-bit binary strings, then we can pick randomly with very little danger of repetition. Let’s push some numbers through and see why this is.</p>
<p>What if we pick a million different values every second, and do this every second of every day for 16 years? What is the probability that we see a duplicate value? Our “powers of two approximation” techniques are very useful here: A million is about <span class="math inline">\(2^{20}\)</span>, and the number of seconds in 16 years is about <span class="math inline">\(2^{4}\cdot 2^{25}=2^{29}\)</span>. Therefore, we are picking <span class="math inline">\(n=2^{49}\)</span> values, which is around 500 trillion values. That’s a lot! But there are <span class="math inline">\(d=2^{128}\)</span> possibilities, which is astronomically large. Using our approximation formula, the probability that we see any repeated value is roughly</p>
<p><span class="math display">\[1-e^{-n^2/(2d)}=1-e^{-(2^{49})^2/(2\cdot 2^{128})}
  = 1-e^{-2^{98}/2^{129}}
  = 1-e^{-1/2^{31}} . \]</span></p>
<p>Since <span class="math inline">\(1/2^{31}\)</span> is so close to zero, <span class="math inline">\(e^{-1/2^{31}}\)</span> is very close to 1, making the probability very small. But how small?</p>
<p>A simple result using the Taylor series expansion of <span class="math inline">\(e^{-x}\)</span> shows that when <span class="math inline">\(x\)</span> is small, <span class="math inline">\(1-e^{-x}\)</span> is approximately <span class="math inline">\(x\)</span>, with an error of less than 1% when <span class="math inline">\(x\leq 0.02\)</span>. Our “<span class="math inline">\(x\)</span>” is <em>much</em> smaller than 0.02, so the probability of a duplicate is very close to <span class="math inline">\(1/2^{31}\)</span>, or roughly 1 in 2 billion. That’s actually several times lower than the odds of winning the Powerball jackpot with a single random ticket! So you can run this system, picking a million values every second for 16 years, with almost no danger of repetition.</p>
<hr />
<p>Next, let’s turn the problem around and ask how many samples we have to take before the probability of seeing a duplicate value exceeds <span class="math inline">\(1/2\)</span>. In other words, for a given <span class="math inline">\(d\)</span> we want to find <span class="math inline">\(n\)</span> such that <span class="math inline">\(e^{-n^2/(2d)}=1/2\)</span>. Doing the algebra to solve for <span class="math inline">\(n\)</span>, we get</p>
<p><span class="math display">\[ e^{-n^2/(2d)} = \frac{1}{2} \iff
   e^{n^2/(2d)} = 2 \iff
   \frac{n^2}{2d} = \ln 2 \iff
   n^2 = (2 \ln 2) d \iff
   n = \sqrt{2\ln 2} \cdot \sqrt{d}, \]</span></p>
<p>so we need <span class="math inline">\(n\approx 1.177\sqrt{d}\)</span>. Returning to the original birthday problem and plugging in <span class="math inline">\(d=365\)</span> we get <span class="math inline">\(n\approx 22.5\)</span>. Since we discovered earlier that the “break-even point” for two people having the same birthday is between 22 and 23 people, this is excellent!</p>
<p>For crypto-sized numbers, we generally ignore the constant multiple of 1.177, since it is so close to 1, and use <span class="math inline">\(n=\sqrt{d}\)</span> for an approximate number of samples required before we are likely to see a duplicate.</p>
<hr />
<p><strong>Example 2.4:</strong> Again picking 128-bit values at random, how many would we need to select before we’re likely to see a duplicate? Taking the square root, this would require approximately <span class="math inline">\(\sqrt{2^{128}}=2^{64}\)</span> samples — about 16 billion billion.</p>
<p>Let’s double-check that answer by going through our duplicate probability formula with <span class="math inline">\(d=2^{128}\)</span> and <span class="math inline">\(n=2^{64}\)</span>. With these values, the probability of a duplicate is approximately</p>
<p><span class="math display">\[1-e^{-n^2/(2d)}
  = 1-e^{-(2^{64})^2/(2\cdot 2^{128})}
  = 1-e^{-2^{128}/2^{129}}
  = 1-e^{-(1/2)}
  \approx 0.3935 . \]</span></p>
<p>So there’s roughly a <span class="math inline">\(4/10\)</span> probability of seeing a duplicate, which is pretty close to our goal of <span class="math inline">\(1/2\)</span> probability, verifying our earlier estimate.</p>
<p>This exact problem comes up a lot in cryptography, including calculating the probability of duplicate initialization vectors for ciphers, the probability of an attack called a “meet-in-the-middle” attack working, the probability of seeing collisions in hash functions, and more. You should be very comfortable with the basic rule of thumb that for a sample space of size <span class="math inline">\(d\)</span>, to have a decent probability of finding a duplicate with random samples you need roughly <span class="math inline">\(\sqrt{d}\)</span> samples.</p>
<hr />
<h4 id="randomness-in-practice-sources-of-randomness">2.3 Randomness in Practice – Sources of Randomness</h4>
<p>In the sections above, we discussed randomness as an abstract mathematical concept, with applied examples using simple physical processes like flipping coins and rolling dice, but we are interested in implementing things in computer systems. Where does randomness come from there? The computer certainly doesn’t sit there with a coin, flipping it over and over. The generic name for what we want is a “random number generator,” or RNG, so how do we make a RNG?</p>
<p>The answer to this question is more difficult than it seems. One warning up front, which cannot be repeated too many times to people dealing with security: Anything you compute with a standard, deterministic process, from given start values, is <em>not</em> random. It doesn’t matter how complex the process is, or how random-<em>looking</em> the output is, it is not random. No matter how many statistical tests it can pass, it’s still <em>not</em> random. For example, there is a function in the standard C library named <code>rand()</code>, which is a pseudorandom number generator (always referred to as a PRNG, where the “P” is <em>very</em> important), and the output of <code>rand()</code> “looks random” – at least if you don’t look too carefully. This function is good enough for random-looking behavior in simple games, but such values are <em>not</em> random, and in high-stakes games the <code>rand()</code> function isn’t “random enough” for even basic unpredictability. Standard PRNGs in other programming languages (such as the <code>java.util.Random</code> class in Java) are no better. Using such a PRNG in a cryptographic protocol is absolutely disastrous, and you can end up with no security at all. There are ways to do pseudorandom number generation that are secure in a cryptographic setting, which we call “cryptographically secure pseudorandom number generation,” and Java does have a separate <code>SecureRandom</code> class that can provide cryptographically secure pseudorandom number generation. There are many pitfalls with attempting to make or use a cryptographically secure pseudorandom number generator, and we will devote a full section to this later. And even with a properly-used cryptographically secure PRNG, while perhaps secure enough in most settings, you are reminded again: these are <em>not</em> random numbers! The overarching truth, that anything you compute with a deterministic algorithm is not random, was summed up in a pithy quote from John von Neumann (one of the “founding fathers” of Computer Science), when he said</p>
<blockquote>
<p><em>Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin. – John von Neumann, 1951</em></p>
</blockquote>
<p>Since you can’t <em>compute</em> random values, they need to come from some other source, such as a physical process. There have been various approaches to this over the years, including a semi-serious random number generator made by engineers at Silicon Graphics using pictures of lava lamps (this was known as <a href="https://en.wikipedia.org/wiki/Lavarand">Lavarand</a>). More practically, there are several higher-bandwidth sources of randomness that modern systems use, that we describe below. For all sources of randomness, a user needs to think about whether the source is uniform, because in most cases it is not. A source of random bits in which the 0/1 outcomes don’t have exactly the same probability is called a “biased RNG.” Fortunately, there are well-understood techniques for taking a biased input and processing it to remove bias, and when these are used the user (after bias removal) sees is a nice, uniformly-distributed random source.</p>
<p><em>Hardware random number generators:</em> It is possible to build hardware devices that measure random electrical/thermal “noise” in a circuit or measure various actions that result from randomness in quantum physics. If a computer can read from such a device, then it has a source of true randomness. Over the past few decades, there have been hardware random number generators built into all sorts of devices, including devices designed specifically for cryptography, security chips such as Trusted Platform Modules (TPMs), general-purpose computer support chipsets, and even directly into computer CPUs. All modern Intel and AMD CPUs have a hardware random number built in, which is accessible through the <code>RDRAND</code> assembly language instruction.</p>
<p><em>Gathering system entropy:</em> As an alternative to devices built specifically to produce random bits, a computer can measure fluctuating physical processes that it has access to. Computer systems respond to various events that have variable timing, such as keys being pressed on the keyboard, network packets arriving, and hard drives seeking to a particular track. Measuring these events with a high precision timer, the least significant bits of the times (e.g., the “microseconds” part of the time between keystrokes) provides significant randomness. When the PGP email-encryption program was initially released in the early 1990’s, key generation required the user to type for about a minute while the program collected randomness from the keystroke timings. These days, all modern operating systems support this idea directly, constantly measuring these events to feed a pool of random bits, which can be provided to the user through various interfaces. For example, in Linux the <code>/dev/random</code> device provides these random numbers, and can be opened and read by any program. This is a blocking device, however, and so programs that try to read more randomness than is available will stall until the system collects enough randomness to provide to the user. The alternative is the non-blocking <code>/dev/urandom</code> device, which can provide cryptographically secure pseudorandom values when true random values are not available. Unfortunately, it is difficult to get enough randomness this way in non-traditional computing systems such as embedded devices, which don’t have a keyboard or hard-drives. This has resulted in some security issues, as explored in the following 2013 research paper:</p>
<blockquote>
<p>Keaton Mowery, Michael Wei, David Kohlbrenner, Hovav Shacham, Steven Swanson, “<a href="https://doi.org/10.1109/SP.2013.46">Welcome to the Entropics: Boot-Time Entropy in Embedded Devices</a>,” <em>IEEE Symposium on Security and Privacy</em>, 2013, pp. 589-603.</p>
</blockquote>
<p>Several times in this section, we have referred to “sufficient randomness” or similar notions. Is there a way to measure randomness and reason about it? Yes! That’s the topic of “entropy,” which we cover in the next section.</p>
<p> </p>
<p><a href="randomness.html">Up: Contents</a>      <a href="rand_estimation.html">Prev: Estimation with Powers of Two</a>      <a href="rand_entropy.html">Next: Entropy</a></p>
<hr />
<div style="font-size: 80%;">
<p>© Copyright 2020–2025, <a href="https://home.uncg.edu/cmp/faculty/srtate/index.html">Stephen R. Tate</a><br/> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</p>
</div>
<p> </p>

    </div>

        <!-- jQuery CDN - Slim version (=without AJAX) -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <!-- Bootstrap JS -->
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js" integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm" crossorigin="anonymous"></script>



  </body>

</html>
